{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install and import libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azureml-contrib-fairness --q\n",
    "!pip install fairlearn --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from fairlearn.widget import FairlearnDashboard\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: 27816, 2: 3124, 1: 1039, 0: 311, 3: 271}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the census dataset\n",
    "X_raw, Y = shap.datasets.adult()\n",
    "X_raw[\"Race\"].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Workclass</th>\n",
       "      <th>Education-Num</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Race</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Capital Gain</th>\n",
       "      <th>Capital Loss</th>\n",
       "      <th>Hours per week</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.0</td>\n",
       "      <td>7</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.0</td>\n",
       "      <td>6</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Workclass  Education-Num  Marital Status  Occupation  Relationship  \\\n",
       "0  39.0          7           13.0               4           1             0   \n",
       "1  50.0          6           13.0               2           4             4   \n",
       "\n",
       "   Race  Sex  Capital Gain  Capital Loss  Hours per week  Country  \n",
       "0     4    1        2174.0           0.0            40.0       39  \n",
       "1     4    1           0.0           0.0            13.0       39  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_raw.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False,  True]), array([24720,  7841]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Y, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the class is unbalanced and in the majority of the cases, people is denied to get a loan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Some feature transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Separate the \"sex\" and \"race\" sensitive features out and drop them from the main data prior to training your model\n",
    "A = X_raw[['Sex','Race']]\n",
    "X = X_raw.drop(labels=['Sex', 'Race'],axis = 1)\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_scaled = sc.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Perform some standard data preprocessing steps to convert the data into a format suitable for the ML algorithms\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Split the dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test, A_train, A_test = train_test_split(X_scaled, \n",
    "                                                    Y, \n",
    "                                                    A,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=Y)\n",
    "\n",
    "# Work around indexing issue\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "A_train = A_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "A_test = A_test.reset_index(drop=True)\n",
    "\n",
    "# Improve labels\n",
    "A_test.Sex.loc[(A_test['Sex'] == 0)] = 'female'\n",
    "A_test.Sex.loc[(A_test['Sex'] == 1)] = 'male'\n",
    "\n",
    "\n",
    "A_test.Race.loc[(A_test['Race'] == 0)] = 'Amer-Indian-Eskimo'\n",
    "A_test.Race.loc[(A_test['Race'] == 1)] = 'Asian-Pac-Islander'\n",
    "A_test.Race.loc[(A_test['Race'] == 2)] = 'Black'\n",
    "A_test.Race.loc[(A_test['Race'] == 3)] = 'Other'\n",
    "A_test.Race.loc[(A_test['Race'] == 4)] = 'White'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Workclass</th>\n",
       "      <th>Education-Num</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Capital Gain</th>\n",
       "      <th>Capital Loss</th>\n",
       "      <th>Hours per week</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.435581</td>\n",
       "      <td>-2.65732</td>\n",
       "      <td>-0.03136</td>\n",
       "      <td>0.921634</td>\n",
       "      <td>-1.554283</td>\n",
       "      <td>-0.281263</td>\n",
       "      <td>-0.14592</td>\n",
       "      <td>-0.21666</td>\n",
       "      <td>-0.845327</td>\n",
       "      <td>0.291569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.837109</td>\n",
       "      <td>0.09005</td>\n",
       "      <td>-0.03136</td>\n",
       "      <td>-0.406212</td>\n",
       "      <td>0.101036</td>\n",
       "      <td>0.856261</td>\n",
       "      <td>-0.14592</td>\n",
       "      <td>-0.21666</td>\n",
       "      <td>-0.035429</td>\n",
       "      <td>0.291569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age  Workclass  Education-Num  Marital Status  Occupation  \\\n",
       "0 -1.435581   -2.65732       -0.03136        0.921634   -1.554283   \n",
       "1  0.837109    0.09005       -0.03136       -0.406212    0.101036   \n",
       "\n",
       "   Relationship  Capital Gain  Capital Loss  Hours per week   Country  \n",
       "0     -0.281263      -0.14592      -0.21666       -0.845327  0.291569  \n",
       "1      0.856261      -0.14592      -0.21666       -0.035429  0.291569  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Azure ML Workspace Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment = Experiment(workspace=ws, name=\"exp-loan-fairness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg AUC: 0.8801876766884713\n"
     ]
    }
   ],
   "source": [
    "### LogisticRegression\n",
    "run = experiment.start_logging(snapshot_directory=None)\n",
    "run.log(\"model_type\", \"LogisticRegression\")\n",
    "\n",
    "lr_predictor = LogisticRegression()\n",
    "lr_predictor.fit(X_train, Y_train)\n",
    "\n",
    "y_pred_log = lr_predictor.predict_proba(X_test)[:, 1]\n",
    "print('LogReg AUC: ' + str(roc_auc_score(Y_test, y_pred_log)))\n",
    "run.log(\"AUC\", roc_auc_score(Y_test, y_pred_log))\n",
    "\n",
    "model_name = \"Model_logregression.pkl\"\n",
    "filename = \"outputs/\" + model_name\n",
    "joblib.dump(value=lr_predictor, filename=filename)\n",
    "run.upload_file(name=model_name, path_or_stream=filename)\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC AUC: 0.8843116113988567\n"
     ]
    }
   ],
   "source": [
    "### Support Vector Machine\n",
    "run = experiment.start_logging(snapshot_directory=None)\n",
    "run.log(\"model_type\", \"SVC\")\n",
    "\n",
    "svm_predictor = SVC(probability=True)\n",
    "svm_predictor.fit(X_train, Y_train)\n",
    "\n",
    "y_pred_svc = svm_predictor.predict_proba(X_test)[:, 1]\n",
    "print('SVC AUC: ' + str(roc_auc_score(Y_test, y_pred_svc)))\n",
    "run.log(\"AUC\", roc_auc_score(Y_test, y_pred_svc))\n",
    "\n",
    "model_name = \"Model_svc.pkl\"\n",
    "filename = \"outputs/\" + model_name\n",
    "joblib.dump(value=svm_predictor, filename=filename)\n",
    "run.upload_file(name=model_name, path_or_stream=filename)\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree AUC: 0.7733715642475393\n"
     ]
    }
   ],
   "source": [
    "### Decision Tree\n",
    "run = experiment.start_logging(snapshot_directory=None)\n",
    "run.log(\"model_type\", \"DecisionTree\")\n",
    "\n",
    "dt_predictor = DecisionTreeClassifier()\n",
    "dt_predictor.fit(X_train, Y_train)\n",
    "\n",
    "y_pred_dt = dt_predictor.predict_proba(X_test)[:, 1]\n",
    "print('DecisionTree AUC: ' + str(roc_auc_score(Y_test, y_pred_dt)))\n",
    "run.log(\"AUC\", roc_auc_score(Y_test, y_pred_dt))\n",
    "\n",
    "model_name = \"Model_decisiontree.pkl\"\n",
    "filename = \"outputs/\" + model_name\n",
    "joblib.dump(value=dt_predictor, filename=filename)\n",
    "run.upload_file(name=model_name, path_or_stream=filename)\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest AUC: 0.9159503905202122\n"
     ]
    }
   ],
   "source": [
    "### Random Forest\n",
    "run = experiment.start_logging(snapshot_directory=None)\n",
    "run.log(\"model_type\", \"RandomForest\")\n",
    "\n",
    "rf_predictor = RandomForestClassifier(max_depth= 90, max_features=3, min_samples_leaf=4, min_samples_split=10, n_estimators=100)\n",
    "rf_predictor.fit(X_train, Y_train)\n",
    "\n",
    "y_pred_rf = rf_predictor.predict_proba(X_test)[:, 1]\n",
    "print('RandomForest AUC: ' + str(roc_auc_score(Y_test, y_pred_rf)))\n",
    "run.log(\"AUC\", roc_auc_score(Y_test, y_pred_rf))\n",
    "\n",
    "model_name = \"Model_randomforest.pkl\"\n",
    "filename = \"outputs/\" + model_name\n",
    "joblib.dump(value=rf_predictor, filename=filename)\n",
    "run.upload_file(name=model_name, path_or_stream=filename)\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoosting accuracy: 0.9117854434493717\n"
     ]
    }
   ],
   "source": [
    "### Gradient Boosting\n",
    "run = experiment.start_logging(snapshot_directory=None)\n",
    "run.log(\"model_type\", \"GradientBoosting\")\n",
    "\n",
    "gb_predictor = GradientBoostingClassifier(learning_rate=0.01, \n",
    "                                     max_depth=3, \n",
    "                                     max_features=0.1,\n",
    "                                     min_samples_leaf=16, \n",
    "                                     n_estimators=1000)\n",
    "gb_predictor.fit(X_train, Y_train)\n",
    "\n",
    "y_pred_gbr = gb_predictor.predict_proba(X_test)[:, 1]\n",
    "print('GradientBoosting accuracy: ' + str(roc_auc_score(Y_test, y_pred_gbr)))\n",
    "run.log(\"Accuracy\", roc_auc_score(Y_test, y_pred_gbr))\n",
    "\n",
    "model_name = \"Model_gradientboosting.pkl\"\n",
    "filename = \"outputs/\" + model_name\n",
    "joblib.dump(value=gb_predictor, filename=filename)\n",
    "run.upload_file(name=model_name, path_or_stream=filename)\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost accuracy: 0.9256579388580509\n"
     ]
    }
   ],
   "source": [
    "### XGBoost\n",
    "run = experiment.start_logging(snapshot_directory=None)\n",
    "run.log(\"model_type\", \"XGBoost\")\n",
    "\n",
    "xgb_predictor = XGBClassifier(colsample_bytree = 0.3,\n",
    "                        learning_rate = 0.03,\n",
    "                        max_depth = 5,\n",
    "                        min_child_weight = 5, \n",
    "                        n_estimators = 500,\n",
    "                        objective ='binary:logistic',\n",
    "                        metric = 'auc')\n",
    "xgb_predictor.fit(X_train, Y_train)\n",
    "\n",
    "y_pred_xgbm = xgb_predictor.predict_proba(X_test)[:, 1]\n",
    "print('XGBoost accuracy: ' + str(roc_auc_score(Y_test, y_pred_xgbm)))\n",
    "run.log(\"Accuracy\", roc_auc_score(Y_test, y_pred_xgbm))\n",
    "\n",
    "model_name = \"Model_xgboost.pkl\"\n",
    "filename = \"outputs/\" + model_name\n",
    "joblib.dump(value=xgb_predictor, filename=filename)\n",
    "run.upload_file(name=model_name, path_or_stream=filename)\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM accuracy: 0.9262470079032624\n"
     ]
    }
   ],
   "source": [
    "### LightGBM\n",
    "run = experiment.start_logging(snapshot_directory=None)\n",
    "run.log(\"model_type\", \"LightGBM\")\n",
    "\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': { 'AUC' },\n",
    "    'num_leaves': 32,\n",
    "    'max_depth': 3,\n",
    "    'min_data_in_leaf': 16,\n",
    "    'bagging_freq': 1,\n",
    "    'feature_fraction': 0.7,\n",
    "    'verbose': 1,\n",
    "    'is_unbalance':True,\n",
    "    'learning_rate': 0.005,\n",
    "    'bagging_fraction': 0.9,\n",
    "}\n",
    "\n",
    "train_set = lgb.Dataset(X_train, Y_train)\n",
    "validation_sets = lgb.Dataset(X_test, Y_test, reference=train_set)\n",
    "\n",
    "lgbm_predictor = lgb.train(\n",
    "    params,\n",
    "    train_set,\n",
    "    num_boost_round=10000,\n",
    "    valid_sets=validation_sets,\n",
    "    early_stopping_rounds=500,\n",
    "    verbose_eval=False\n",
    "    )\n",
    "\n",
    "y_pred_lgbm = lgbm_predictor.predict(X_test)\n",
    "print('LightGBM accuracy: ' + str(roc_auc_score(Y_test, y_pred_lgbm)))\n",
    "run.log(\"Accuracy\", roc_auc_score(Y_test, y_pred_lgbm))\n",
    "\n",
    "model_name = \"Model_lgbm.pkl\"\n",
    "filename = \"outputs/\" + model_name\n",
    "joblib.dump(value=lgbm_predictor, filename=filename)\n",
    "run.upload_file(name=model_name, path_or_stream=filename)\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Registering the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "from azureml.core import  Model\n",
    "\n",
    "def register_model(name, model):\n",
    "    print(\"Registering \", name)\n",
    "    model_path = \"outputs/{0}.pkl\".format(name)\n",
    "    joblib.dump(value=model, filename=model_path)\n",
    "    registered_model = Model.register(model_path=model_path,\n",
    "                                      model_name=name,\n",
    "                                      workspace=ws)\n",
    "    print(\"Registered \", registered_model.id)\n",
    "    return registered_model.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering  Model_logregression\n",
      "Registering model Model_logregression\n",
      "Registered  Model_logregression:2\n",
      "Registering  Model_svc\n",
      "Registering model Model_svc\n",
      "Registered  Model_svc:2\n",
      "Registering  Model_decisiontree\n",
      "Registering model Model_decisiontree\n",
      "Registered  Model_decisiontree:2\n",
      "Registering  Model_randomforest\n",
      "Registering model Model_randomforest\n",
      "Registered  Model_randomforest:2\n",
      "Registering  Model_gradientboosting\n",
      "Registering model Model_gradientboosting\n",
      "Registered  Model_gradientboosting:2\n",
      "Registering  Model_xgboost\n",
      "Registering model Model_xgboost\n",
      "Registered  Model_xgboost:2\n",
      "Registering  Model_lgbm\n",
      "Registering model Model_lgbm\n",
      "Registered  Model_lgbm:2\n"
     ]
    }
   ],
   "source": [
    "model_dict = {}\n",
    "\n",
    "lr_reg_id = register_model(\"Model_logregression\", lr_predictor)\n",
    "model_dict[lr_reg_id] = lr_predictor\n",
    "svm_reg_id = register_model(\"Model_svc\", svm_predictor)\n",
    "model_dict[svm_reg_id] = svm_predictor\n",
    "dt_reg_id = register_model(\"Model_decisiontree\", dt_predictor)\n",
    "model_dict[dt_reg_id] = dt_predictor\n",
    "rf_reg_id = register_model(\"Model_randomforest\", rf_predictor)\n",
    "model_dict[rf_reg_id] = rf_predictor\n",
    "gb_reg_id = register_model(\"Model_gradientboosting\", gb_predictor)\n",
    "model_dict[gb_reg_id] = gb_predictor\n",
    "xgb_reg_id = register_model(\"Model_xgboost\", xgb_predictor)\n",
    "model_dict[xgb_reg_id] = xgb_predictor\n",
    "lgbm_reg_id = register_model(\"Model_lgbm\", lgbm_predictor)\n",
    "model_dict[lgbm_reg_id] = lgbm_predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Generate Fairlearn Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_pred = {}\n",
    "for n, p in model_dict.items():\n",
    "    if(n.split(':')[0]=='Model_lgbm'):\n",
    "        ys_pred[n] = p.predict(X_test)\n",
    "    else:\n",
    "        ys_pred[n] = p.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ys_pred = {}\n",
    "# for n, p in model_dict.items():\n",
    "#     ys_pred[n] = p.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be7af85c741410c94bf5e07da070979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FairlearnWidget(value={'true_y': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<fairlearn.widget._fairlearn_dashboard.FairlearnDashboard at 0x7fcb891345c0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairlearn.widget import FairlearnDashboard\n",
    "\n",
    "FairlearnDashboard(sensitive_features=A_test, \n",
    "                   sensitive_feature_names=['Sex', 'Race'],\n",
    "                   y_true=Y_test.tolist(),\n",
    "                   y_pred=ys_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Upload Fairness Dashboard to Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ys_pred = {}\n",
    "# for n, p in model_dict.items():\n",
    "#     if(n.split(':')[0]=='Model_lgbm'):\n",
    "#         ys_pred[n] = (p.predict(X_test) >= 0.5)*1.0\n",
    "#     else:\n",
    "#         ys_pred[n] = p.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = { 'Race': A_test['Race'], 'Sex': A_test['Sex'] }\n",
    "\n",
    "from fairlearn.metrics._group_metric_set import _create_group_metric_set\n",
    "\n",
    "dash_dict = _create_group_metric_set(y_true=Y_test.astype(float),\n",
    "                                     predictions=ys_pred,\n",
    "                                     sensitive_features=sf,\n",
    "                                     prediction_type='binary_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploaded to id: 4eb20973-bdb7-4533-aa78-169128f584d9\n",
      "\n",
      "\n",
      "Uploaded to id: 3c2c93fb-5859-480c-96dd-5878e435a96a\n",
      "\n",
      "\n",
      "Uploaded to id: 4ed17ce1-215b-424e-bf05-d687f91a2fa0\n",
      "\n",
      "\n",
      "Uploaded to id: da033d5b-79dc-4d17-a75e-70168da4ce3c\n",
      "\n",
      "\n",
      "Uploaded to id: 8c5bf082-2d73-4039-98ee-54412449dc92\n",
      "\n",
      "\n",
      "Uploaded to id: aebcf83a-8cf6-43b3-8e0b-db648ba4996d\n",
      "\n",
      "\n",
      "Uploaded to id: 095b4d00-5253-4eeb-9dda-1042398185cb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azureml.contrib.fairness import upload_dashboard_dictionary, download_dashboard_by_upload_id\n",
    "for i_run in experiment.get_runs():\n",
    "    dashboard_title = \"Fairness Dashboard\"\n",
    "    upload_id = upload_dashboard_dictionary(i_run,\n",
    "                                            dash_dict,\n",
    "                                            dashboard_name=dashboard_title)\n",
    "    print(\"\\nUploaded to id: {0}\\n\".format(upload_id))\n",
    "\n",
    "    downloaded_dict = download_dashboard_by_upload_id(i_run, upload_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(dash_dict == downloaded_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
